{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('user1_data.csv')\n",
    "df_2 = pd.read_csv('user2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_1.drop(['sm','ttime'],axis=1)\n",
    "y1 = df_1[\"sm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df_2.drop(['sm','ttime'],axis=1)\n",
    "y2 = df_2[\"sm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 115.17850581390579\n",
      "Mean Squared Error: 254.86003753802856\n",
      "R^2 Score: 0.9922637229882875\n"
     ]
    }
   ],
   "source": [
    "# random forest with no depth limit\n",
    "# current best \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y1,test_size=0.3,random_state=101)\n",
    "rf = RandomForestRegressor(random_state=101)\n",
    "rf.fit(X_train,y_train)\n",
    "# rf.fit(X1,y1)\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions,squared=False))\n",
    "\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R^2 Score:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 143.41380201602482\n",
      "Mean Squared Error: 369.46027220411855\n"
     ]
    }
   ],
   "source": [
    "# decision tree classifier\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1 = df_1.drop(['sm','ttime'],axis=1)\n",
    "y1 = df_1[\"sm\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y1,test_size=0.2,random_state=42)\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "predictions = dtree.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1755.5256822935173\n",
      "Mean Squared Error: 1951.2148880824834\n"
     ]
    }
   ],
   "source": [
    "# ridge \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X1 = df_1.drop(['sm','ttime'],axis=1)\n",
    "y1 = df_1[\"sm\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y1,test_size=0.2,random_state=42)\n",
    "model = Ridge()\n",
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions,squared=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pm1       pm2       pm3        am        sm        st       lum\n",
      "pm1  1.000000  0.573090  0.552320  0.069879  0.441345 -0.122531  0.047982\n",
      "pm2  0.573090  1.000000  0.922340  0.121761  0.728241 -0.205373  0.069458\n",
      "pm3  0.552320  0.922340  1.000000  0.127813  0.730132 -0.185220  0.080850\n",
      "am   0.069879  0.121761  0.127813  1.000000  0.148441 -0.287205  0.344563\n",
      "sm   0.441345  0.728241  0.730132  0.148441  1.000000 -0.173479  0.082105\n",
      "st  -0.122531 -0.205373 -0.185220 -0.287205 -0.173479  1.000000 -0.215903\n",
      "lum  0.047982  0.069458  0.080850  0.344563  0.082105 -0.215903  1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6848/2268593849.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  corr_mat = df_1.corr()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corr_mat = df_1.corr()\n",
    "print (corr_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 3322.4543 - val_loss: 2560.9080\n",
      "Epoch 2/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1981.1218 - val_loss: 1893.9402\n",
      "Epoch 3/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1867.0162 - val_loss: 1861.7546\n",
      "Epoch 4/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1838.5505 - val_loss: 1812.9290\n",
      "Epoch 5/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1799.2489 - val_loss: 1799.3270\n",
      "Epoch 6/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1770.2373 - val_loss: 1770.9517\n",
      "Epoch 7/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1730.0536 - val_loss: 1690.8304\n",
      "Epoch 8/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1665.0768 - val_loss: 1589.4241\n",
      "Epoch 9/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1571.0598 - val_loss: 1528.4526\n",
      "Epoch 10/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1529.8009 - val_loss: 1426.8267\n",
      "Epoch 11/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1450.3502 - val_loss: 1390.3713\n",
      "Epoch 12/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1452.0420 - val_loss: 1452.5408\n",
      "Epoch 13/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1383.2538 - val_loss: 1322.3153\n",
      "Epoch 14/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1333.1267 - val_loss: 1522.2267\n",
      "Epoch 15/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1286.3856 - val_loss: 1230.9802\n",
      "Epoch 16/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1238.6877 - val_loss: 1191.2286\n",
      "Epoch 17/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1169.6444 - val_loss: 1169.6312\n",
      "Epoch 18/50\n",
      "339/339 [==============================] - 1s 1ms/step - loss: 1112.9766 - val_loss: 1110.0255\n",
      "Epoch 19/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 1045.5542 - val_loss: 1119.3403\n",
      "Epoch 20/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 976.8951 - val_loss: 920.6814\n",
      "Epoch 21/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 954.1521 - val_loss: 898.5422\n",
      "Epoch 22/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 918.7027 - val_loss: 946.5033\n",
      "Epoch 23/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 917.8221 - val_loss: 908.6282\n",
      "Epoch 24/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 905.5688 - val_loss: 843.6014\n",
      "Epoch 25/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 882.6770 - val_loss: 861.0488\n",
      "Epoch 26/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 880.3431 - val_loss: 916.1381\n",
      "Epoch 27/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 851.9125 - val_loss: 817.0129\n",
      "Epoch 28/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 843.4175 - val_loss: 830.9648\n",
      "Epoch 29/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 848.5150 - val_loss: 838.4319\n",
      "Epoch 30/50\n",
      "339/339 [==============================] - 1s 1ms/step - loss: 817.1718 - val_loss: 834.7799\n",
      "Epoch 31/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 808.1924 - val_loss: 829.5479\n",
      "Epoch 32/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 794.7886 - val_loss: 777.3685\n",
      "Epoch 33/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 799.1544 - val_loss: 785.5081\n",
      "Epoch 34/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 788.2355 - val_loss: 759.0443\n",
      "Epoch 35/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 775.2668 - val_loss: 735.2664\n",
      "Epoch 36/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 739.7347 - val_loss: 724.3627\n",
      "Epoch 37/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 753.8378 - val_loss: 721.4495\n",
      "Epoch 38/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 728.6114 - val_loss: 702.0420\n",
      "Epoch 39/50\n",
      "339/339 [==============================] - 1s 3ms/step - loss: 698.2773 - val_loss: 725.8278\n",
      "Epoch 40/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 708.3989 - val_loss: 691.1625\n",
      "Epoch 41/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 696.3773 - val_loss: 656.2220\n",
      "Epoch 42/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 668.9534 - val_loss: 649.4114\n",
      "Epoch 43/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 668.0689 - val_loss: 633.4221\n",
      "Epoch 44/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 649.4709 - val_loss: 599.7791\n",
      "Epoch 45/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 629.8276 - val_loss: 659.5617\n",
      "Epoch 46/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 607.7087 - val_loss: 617.8371\n",
      "Epoch 47/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 604.3909 - val_loss: 585.2930\n",
      "Epoch 48/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 588.7122 - val_loss: 615.5796\n",
      "Epoch 49/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 590.1811 - val_loss: 554.1792\n",
      "Epoch 50/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 574.3237 - val_loss: 577.0395\n",
      "182/182 [==============================] - 0s 1ms/step\n",
      "R^2 Score: 0.8750689804079075\n",
      "Mean Absolute Error: 570.8404917650422\n",
      "Mean Squared Error: 1024.1672374742843\n"
     ]
    }
   ],
   "source": [
    "# Neural Net 92 % for 500 epoch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.losses import Huber \n",
    "from keras.losses import LogCosh \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# load the data\n",
    "# df_1 = pd.read_csv('data.csv')\n",
    "\n",
    "# split into features (X) and target (y)\n",
    "X = df_1.drop(['sm', 'ttime'], axis=1).values\n",
    "y = df_1['sm'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation=None))\n",
    "\n",
    "# compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam') # 533\n",
    "# model.compile(loss='mean_absolute_error', optimizer='adam') # 432 error\n",
    "model.compile(loss=Huber(delta=1.0), optimizer='adam')  # 388\n",
    "# model.compile(loss=LogCosh(), optimizer='adam') # 470\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer=Adam(lr=1)) # 886\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam') # 825\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R^2 Score:\", r2)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "484/484 [==============================] - 2s 2ms/step - loss: 16333019.0000 - mse: 16333019.0000 - val_loss: 5876793.0000 - val_mse: 5876793.0000\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 4533620.0000 - mse: 4533620.0000 - val_loss: 3827276.0000 - val_mse: 3827276.0000\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 3146202.7500 - mse: 3146202.7500 - val_loss: 2458383.0000 - val_mse: 2458383.0000\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 1818523.8750 - mse: 1818523.8750 - val_loss: 1425529.7500 - val_mse: 1425529.7500\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 1219734.2500 - mse: 1219734.2500 - val_loss: 1137975.7500 - val_mse: 1137975.7500\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 1038131.5625 - mse: 1038131.5625 - val_loss: 1009258.8750 - val_mse: 1009258.8750\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 951753.7500 - mse: 951753.7500 - val_loss: 942411.5625 - val_mse: 942411.5625\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 899445.4375 - mse: 899445.4375 - val_loss: 902046.8750 - val_mse: 902046.8750\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 868404.4375 - mse: 868404.4375 - val_loss: 868956.6250 - val_mse: 868956.6250\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 842654.2500 - mse: 842654.2500 - val_loss: 860651.0000 - val_mse: 860651.0000\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 823171.1250 - mse: 823171.1250 - val_loss: 824998.1250 - val_mse: 824998.1250\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 802695.1875 - mse: 802695.1875 - val_loss: 824820.4375 - val_mse: 824820.4375\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 780136.8125 - mse: 780136.8125 - val_loss: 779937.8750 - val_mse: 779937.8750\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 749319.8125 - mse: 749319.8125 - val_loss: 748273.0625 - val_mse: 748273.0625\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 714073.3125 - mse: 714073.3125 - val_loss: 710706.6250 - val_mse: 710706.6250\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 678640.2500 - mse: 678640.2500 - val_loss: 685633.7500 - val_mse: 685633.7500\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 647229.3125 - mse: 647229.3125 - val_loss: 652256.5000 - val_mse: 652256.5000\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 618695.0625 - mse: 618695.0625 - val_loss: 620075.3750 - val_mse: 620075.3750\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 591977.8750 - mse: 591977.8750 - val_loss: 595435.1875 - val_mse: 595435.1875\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 565171.8750 - mse: 565171.8750 - val_loss: 571797.1875 - val_mse: 571797.1875\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 538614.9375 - mse: 538614.9375 - val_loss: 539570.5000 - val_mse: 539570.5000\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 511476.5938 - mse: 511476.5938 - val_loss: 511586.6250 - val_mse: 511586.6250\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 487703.7812 - mse: 487703.7812 - val_loss: 490346.4062 - val_mse: 490346.4062\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 461707.1875 - mse: 461707.1875 - val_loss: 459892.2500 - val_mse: 459892.2500\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 436842.1562 - mse: 436842.1562 - val_loss: 435237.4375 - val_mse: 435237.4375\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 412582.0000 - mse: 412582.0000 - val_loss: 413675.5312 - val_mse: 413675.5312\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 391137.0312 - mse: 391137.0312 - val_loss: 388053.0938 - val_mse: 388053.0938\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 369927.6875 - mse: 369927.6875 - val_loss: 375939.3125 - val_mse: 375939.3125\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 348144.1562 - mse: 348144.1562 - val_loss: 350231.9062 - val_mse: 350231.9062\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 329869.9375 - mse: 329869.9375 - val_loss: 332096.0625 - val_mse: 332096.0625\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 311029.1250 - mse: 311029.1250 - val_loss: 313742.6562 - val_mse: 313742.6562\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 292301.4375 - mse: 292301.4375 - val_loss: 288319.2188 - val_mse: 288319.2188\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 275997.2500 - mse: 275997.2500 - val_loss: 275727.3750 - val_mse: 275727.3750\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 260975.6562 - mse: 260975.6562 - val_loss: 257764.9219 - val_mse: 257764.9219\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 248716.3594 - mse: 248716.3594 - val_loss: 248695.2969 - val_mse: 248695.2969\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 236155.1250 - mse: 236155.1250 - val_loss: 233688.8750 - val_mse: 233688.8750\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 224478.5000 - mse: 224478.5000 - val_loss: 225880.7031 - val_mse: 225880.7031\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 216300.7031 - mse: 216300.7031 - val_loss: 215162.5469 - val_mse: 215162.5469\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 208208.0625 - mse: 208208.0625 - val_loss: 211412.4688 - val_mse: 211412.4688\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 202354.5156 - mse: 202354.5156 - val_loss: 202722.5469 - val_mse: 202722.5469\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 196931.0156 - mse: 196931.0156 - val_loss: 200305.2500 - val_mse: 200305.2500\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 193440.3750 - mse: 193440.3750 - val_loss: 194357.8125 - val_mse: 194357.8125\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 188352.3906 - mse: 188352.3906 - val_loss: 191635.5312 - val_mse: 191635.5312\n",
      "Epoch 44/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 185414.6562 - mse: 185414.6562 - val_loss: 183717.5625 - val_mse: 183717.5625\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 180078.4688 - mse: 180078.4688 - val_loss: 187698.5000 - val_mse: 187698.5000\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 176430.9844 - mse: 176430.9844 - val_loss: 175828.3125 - val_mse: 175828.3125\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 174605.0938 - mse: 174605.0938 - val_loss: 172863.0469 - val_mse: 172863.0469\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 171540.6562 - mse: 171540.6562 - val_loss: 173365.0625 - val_mse: 173365.0625\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 169116.8750 - mse: 169116.8750 - val_loss: 167846.2344 - val_mse: 167846.2344\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 166502.5000 - mse: 166502.5000 - val_loss: 169166.4531 - val_mse: 169166.4531\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 164908.7344 - mse: 164908.7344 - val_loss: 167459.6250 - val_mse: 167459.6250\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 162186.6562 - mse: 162186.6562 - val_loss: 164371.7812 - val_mse: 164371.7812\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 161338.0938 - mse: 161338.0938 - val_loss: 163030.0312 - val_mse: 163030.0312\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 158936.2188 - mse: 158936.2188 - val_loss: 162634.8750 - val_mse: 162634.8750\n",
      "Epoch 55/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 156545.1562 - mse: 156545.1562 - val_loss: 156128.3594 - val_mse: 156128.3594\n",
      "Epoch 56/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 154328.3281 - mse: 154328.3281 - val_loss: 161050.9062 - val_mse: 161050.9062\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 152689.5000 - mse: 152689.5000 - val_loss: 153523.0156 - val_mse: 153523.0156\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 151757.0938 - mse: 151757.0938 - val_loss: 152829.6406 - val_mse: 152829.6406\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 149479.2500 - mse: 149479.2500 - val_loss: 152730.9531 - val_mse: 152730.9531\n",
      "Epoch 60/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 148184.5781 - mse: 148184.5781 - val_loss: 150023.6719 - val_mse: 150023.6719\n",
      "Epoch 61/500\n",
      "484/484 [==============================] - 1s 3ms/step - loss: 146740.3750 - mse: 146740.3750 - val_loss: 150451.3281 - val_mse: 150451.3281\n",
      "Epoch 62/500\n",
      "484/484 [==============================] - 1s 3ms/step - loss: 145357.3906 - mse: 145357.3906 - val_loss: 147853.2031 - val_mse: 147853.2031\n",
      "Epoch 63/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 143502.5625 - mse: 143502.5625 - val_loss: 145249.3594 - val_mse: 145249.3594\n",
      "Epoch 64/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 142814.6562 - mse: 142814.6562 - val_loss: 149530.8125 - val_mse: 149530.8125\n",
      "Epoch 65/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 142771.8906 - mse: 142771.8906 - val_loss: 149157.1875 - val_mse: 149157.1875\n",
      "Epoch 66/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 139996.3281 - mse: 139996.3281 - val_loss: 150952.0000 - val_mse: 150952.0000\n",
      "Epoch 67/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 139855.3438 - mse: 139855.3438 - val_loss: 146595.8125 - val_mse: 146595.8125\n",
      "Epoch 68/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 137818.8438 - mse: 137818.8438 - val_loss: 143833.2812 - val_mse: 143833.2812\n",
      "Epoch 69/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 137176.8125 - mse: 137176.8125 - val_loss: 141712.4531 - val_mse: 141712.4531\n",
      "Epoch 70/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 135867.0625 - mse: 135867.0625 - val_loss: 142769.4531 - val_mse: 142769.4531\n",
      "Epoch 71/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 135094.7188 - mse: 135094.7188 - val_loss: 141519.2656 - val_mse: 141519.2656\n",
      "Epoch 72/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 134057.5156 - mse: 134057.5156 - val_loss: 137711.3281 - val_mse: 137711.3281\n",
      "Epoch 73/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 132417.3906 - mse: 132417.3906 - val_loss: 144105.5312 - val_mse: 144105.5312\n",
      "Epoch 74/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 131477.5781 - mse: 131477.5781 - val_loss: 134873.2188 - val_mse: 134873.2188\n",
      "Epoch 75/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 130335.1641 - mse: 130335.1641 - val_loss: 134169.2188 - val_mse: 134169.2188\n",
      "Epoch 76/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 129263.8672 - mse: 129263.8672 - val_loss: 132542.7969 - val_mse: 132542.7969\n",
      "Epoch 77/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 128204.1484 - mse: 128204.1484 - val_loss: 131767.4062 - val_mse: 131767.4062\n",
      "Epoch 78/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 127298.9688 - mse: 127298.9688 - val_loss: 133541.0000 - val_mse: 133541.0000\n",
      "Epoch 79/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 126048.7578 - mse: 126048.7578 - val_loss: 130351.2578 - val_mse: 130351.2578\n",
      "Epoch 80/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 125847.1719 - mse: 125847.1719 - val_loss: 129162.5156 - val_mse: 129162.5156\n",
      "Epoch 81/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 124787.0391 - mse: 124787.0391 - val_loss: 131092.6250 - val_mse: 131092.6250\n",
      "Epoch 82/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 123980.7812 - mse: 123980.7812 - val_loss: 130458.9688 - val_mse: 130458.9688\n",
      "Epoch 83/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 122536.8906 - mse: 122536.8906 - val_loss: 132318.0469 - val_mse: 132318.0469\n",
      "Epoch 84/500\n",
      "484/484 [==============================] - 1s 3ms/step - loss: 122282.6172 - mse: 122282.6172 - val_loss: 127991.2266 - val_mse: 127991.2266\n",
      "Epoch 85/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 121172.0156 - mse: 121172.0156 - val_loss: 127472.9141 - val_mse: 127472.9141\n",
      "Epoch 86/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 120618.8516 - mse: 120618.8516 - val_loss: 127987.3203 - val_mse: 127987.3203\n",
      "Epoch 87/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 119328.8828 - mse: 119328.8828 - val_loss: 128419.3984 - val_mse: 128419.3984\n",
      "Epoch 88/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 119567.8359 - mse: 119567.8359 - val_loss: 126364.0625 - val_mse: 126364.0625\n",
      "Epoch 89/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 118223.5391 - mse: 118223.5391 - val_loss: 126110.0547 - val_mse: 126110.0547\n",
      "Epoch 90/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 117724.1875 - mse: 117724.1875 - val_loss: 121912.6641 - val_mse: 121912.6641\n",
      "Epoch 91/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 116776.2500 - mse: 116776.2500 - val_loss: 125585.8672 - val_mse: 125585.8672\n",
      "Epoch 92/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 117005.3516 - mse: 117005.3516 - val_loss: 122286.4688 - val_mse: 122286.4688\n",
      "Epoch 93/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 115647.3047 - mse: 115647.3047 - val_loss: 121838.1562 - val_mse: 121838.1562\n",
      "Epoch 94/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 116101.0000 - mse: 116101.0000 - val_loss: 120641.6094 - val_mse: 120641.6094\n",
      "Epoch 95/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 114502.0234 - mse: 114502.0234 - val_loss: 120247.0000 - val_mse: 120247.0000\n",
      "Epoch 96/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 114285.3516 - mse: 114285.3516 - val_loss: 124574.0156 - val_mse: 124574.0156\n",
      "Epoch 97/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 114055.2344 - mse: 114055.2344 - val_loss: 121310.0234 - val_mse: 121310.0234\n",
      "Epoch 98/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 113995.2266 - mse: 113995.2266 - val_loss: 123457.2578 - val_mse: 123457.2578\n",
      "Epoch 99/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 112634.4219 - mse: 112634.4219 - val_loss: 127164.0234 - val_mse: 127164.0234\n",
      "Epoch 100/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 112087.5000 - mse: 112087.5000 - val_loss: 120584.0156 - val_mse: 120584.0156\n",
      "Epoch 101/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 111039.1016 - mse: 111039.1016 - val_loss: 123814.7344 - val_mse: 123814.7344\n",
      "Epoch 102/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 111452.5703 - mse: 111452.5703 - val_loss: 120403.1797 - val_mse: 120403.1797\n",
      "Epoch 103/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 109597.0391 - mse: 109597.0391 - val_loss: 121946.2500 - val_mse: 121946.2500\n",
      "Epoch 104/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 110134.5000 - mse: 110134.5000 - val_loss: 117356.0391 - val_mse: 117356.0391\n",
      "Epoch 105/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 108630.2578 - mse: 108630.2578 - val_loss: 119819.2344 - val_mse: 119819.2344\n",
      "Epoch 106/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 108852.2656 - mse: 108852.2656 - val_loss: 118934.9219 - val_mse: 118934.9219\n",
      "Epoch 107/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 108272.0469 - mse: 108272.0469 - val_loss: 117135.8906 - val_mse: 117135.8906\n",
      "Epoch 108/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 107831.6328 - mse: 107831.6328 - val_loss: 120499.4531 - val_mse: 120499.4531\n",
      "Epoch 109/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 107575.4844 - mse: 107575.4844 - val_loss: 116497.3203 - val_mse: 116497.3203\n",
      "Epoch 110/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 106759.7656 - mse: 106759.7656 - val_loss: 115059.9375 - val_mse: 115059.9375\n",
      "Epoch 111/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 105991.0625 - mse: 105991.0625 - val_loss: 114380.2578 - val_mse: 114380.2578\n",
      "Epoch 112/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 106225.6719 - mse: 106225.6719 - val_loss: 115873.6719 - val_mse: 115873.6719\n",
      "Epoch 113/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 105417.5000 - mse: 105417.5000 - val_loss: 115777.8125 - val_mse: 115777.8125\n",
      "Epoch 114/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 105633.8203 - mse: 105633.8203 - val_loss: 114043.9297 - val_mse: 114043.9297\n",
      "Epoch 115/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 104627.3672 - mse: 104627.3672 - val_loss: 118676.4375 - val_mse: 118676.4375\n",
      "Epoch 116/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 104575.1172 - mse: 104575.1172 - val_loss: 114169.6953 - val_mse: 114169.6953\n",
      "Epoch 117/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 103721.0312 - mse: 103721.0312 - val_loss: 114793.1172 - val_mse: 114793.1172\n",
      "Epoch 118/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 103533.2109 - mse: 103533.2109 - val_loss: 118091.2500 - val_mse: 118091.2500\n",
      "Epoch 119/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 104436.0156 - mse: 104436.0156 - val_loss: 115119.3516 - val_mse: 115119.3516\n",
      "Epoch 120/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 102969.8828 - mse: 102969.8828 - val_loss: 117137.9844 - val_mse: 117137.9844\n",
      "Epoch 121/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 102236.4688 - mse: 102236.4688 - val_loss: 112089.2109 - val_mse: 112089.2109\n",
      "Epoch 122/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 102346.6797 - mse: 102346.6797 - val_loss: 111303.6719 - val_mse: 111303.6719\n",
      "Epoch 123/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 102677.8203 - mse: 102677.8203 - val_loss: 112060.6484 - val_mse: 112060.6484\n",
      "Epoch 124/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 101973.6953 - mse: 101973.6953 - val_loss: 111538.8672 - val_mse: 111538.8672\n",
      "Epoch 125/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 102160.9453 - mse: 102160.9453 - val_loss: 113766.1250 - val_mse: 113766.1250\n",
      "Epoch 126/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 101353.2266 - mse: 101353.2266 - val_loss: 112041.6406 - val_mse: 112041.6406\n",
      "Epoch 127/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 102118.7266 - mse: 102118.7266 - val_loss: 113233.2031 - val_mse: 113233.2031\n",
      "Epoch 128/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 101458.9688 - mse: 101458.9688 - val_loss: 112442.1562 - val_mse: 112442.1562\n",
      "Epoch 129/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 100829.5156 - mse: 100829.5156 - val_loss: 114361.4531 - val_mse: 114361.4531\n",
      "Epoch 130/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 100729.7266 - mse: 100729.7266 - val_loss: 111128.5859 - val_mse: 111128.5859\n",
      "Epoch 131/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 100675.3594 - mse: 100675.3594 - val_loss: 113782.6406 - val_mse: 113782.6406\n",
      "Epoch 132/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 99940.8203 - mse: 99940.8203 - val_loss: 110819.4844 - val_mse: 110819.4844\n",
      "Epoch 133/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 100977.8516 - mse: 100977.8516 - val_loss: 110098.0391 - val_mse: 110098.0391\n",
      "Epoch 134/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 99679.6406 - mse: 99679.6406 - val_loss: 116637.4844 - val_mse: 116637.4844\n",
      "Epoch 135/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 99032.4766 - mse: 99032.4766 - val_loss: 110398.3438 - val_mse: 110398.3438\n",
      "Epoch 136/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 98719.9453 - mse: 98719.9453 - val_loss: 108693.0625 - val_mse: 108693.0625\n",
      "Epoch 137/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 98410.6250 - mse: 98410.6250 - val_loss: 112438.0547 - val_mse: 112438.0547\n",
      "Epoch 138/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 99033.5781 - mse: 99033.5781 - val_loss: 113278.0859 - val_mse: 113278.0859\n",
      "Epoch 139/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 99022.1641 - mse: 99022.1641 - val_loss: 111104.7031 - val_mse: 111104.7031\n",
      "Epoch 140/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 97989.0859 - mse: 97989.0859 - val_loss: 109367.5469 - val_mse: 109367.5469\n",
      "Epoch 141/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 98016.9688 - mse: 98016.9688 - val_loss: 110119.7969 - val_mse: 110119.7969\n",
      "Epoch 142/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 98142.4062 - mse: 98142.4062 - val_loss: 109066.5312 - val_mse: 109066.5312\n",
      "Epoch 143/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 98197.3594 - mse: 98197.3594 - val_loss: 108768.1328 - val_mse: 108768.1328\n",
      "Epoch 144/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 97178.1328 - mse: 97178.1328 - val_loss: 124438.4297 - val_mse: 124438.4297\n",
      "Epoch 145/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 97559.7266 - mse: 97559.7266 - val_loss: 109574.1953 - val_mse: 109574.1953\n",
      "Epoch 146/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 97255.1094 - mse: 97255.1094 - val_loss: 108311.4766 - val_mse: 108311.4766\n",
      "Epoch 147/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 96561.0625 - mse: 96561.0625 - val_loss: 109481.6094 - val_mse: 109481.6094\n",
      "Epoch 148/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 97450.3594 - mse: 97450.3594 - val_loss: 110897.7266 - val_mse: 110897.7266\n",
      "Epoch 149/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 96000.0000 - mse: 96000.0000 - val_loss: 109068.3828 - val_mse: 109068.3828\n",
      "Epoch 150/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 96315.2031 - mse: 96315.2031 - val_loss: 110218.7422 - val_mse: 110218.7422\n",
      "Epoch 151/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 95881.1562 - mse: 95881.1562 - val_loss: 115066.1172 - val_mse: 115066.1172\n",
      "Epoch 152/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 95854.7109 - mse: 95854.7109 - val_loss: 109971.3438 - val_mse: 109971.3438\n",
      "Epoch 153/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 95754.4922 - mse: 95754.4922 - val_loss: 108654.1484 - val_mse: 108654.1484\n",
      "Epoch 154/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 95462.4297 - mse: 95462.4297 - val_loss: 107114.2734 - val_mse: 107114.2734\n",
      "Epoch 155/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 95048.7422 - mse: 95048.7422 - val_loss: 106223.1328 - val_mse: 106223.1328\n",
      "Epoch 156/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 94927.8984 - mse: 94927.8984 - val_loss: 106903.8828 - val_mse: 106903.8828\n",
      "Epoch 157/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 95401.6250 - mse: 95401.6250 - val_loss: 106435.4609 - val_mse: 106435.4609\n",
      "Epoch 158/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 94811.7578 - mse: 94811.7578 - val_loss: 108155.3516 - val_mse: 108155.3516\n",
      "Epoch 159/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 94283.9453 - mse: 94283.9453 - val_loss: 107060.9453 - val_mse: 107060.9453\n",
      "Epoch 160/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 94909.7969 - mse: 94909.7969 - val_loss: 106124.3594 - val_mse: 106124.3594\n",
      "Epoch 161/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 93087.9531 - mse: 93087.9531 - val_loss: 107383.1250 - val_mse: 107383.1250\n",
      "Epoch 162/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 93627.2812 - mse: 93627.2812 - val_loss: 104841.1406 - val_mse: 104841.1406\n",
      "Epoch 163/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 93939.3203 - mse: 93939.3203 - val_loss: 106649.6641 - val_mse: 106649.6641\n",
      "Epoch 164/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 93131.6172 - mse: 93131.6172 - val_loss: 104378.8750 - val_mse: 104378.8750\n",
      "Epoch 165/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 93083.6094 - mse: 93083.6094 - val_loss: 104852.9844 - val_mse: 104852.9844\n",
      "Epoch 166/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92830.7656 - mse: 92830.7656 - val_loss: 105646.5469 - val_mse: 105646.5469\n",
      "Epoch 167/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92647.2812 - mse: 92647.2812 - val_loss: 107756.6641 - val_mse: 107756.6641\n",
      "Epoch 168/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92817.7812 - mse: 92817.7812 - val_loss: 105999.2266 - val_mse: 105999.2266\n",
      "Epoch 169/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92973.8750 - mse: 92973.8750 - val_loss: 120993.1875 - val_mse: 120993.1875\n",
      "Epoch 170/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92180.5781 - mse: 92180.5781 - val_loss: 104423.7500 - val_mse: 104423.7500\n",
      "Epoch 171/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92350.6562 - mse: 92350.6562 - val_loss: 105315.0938 - val_mse: 105315.0938\n",
      "Epoch 172/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92233.6328 - mse: 92233.6328 - val_loss: 105081.7031 - val_mse: 105081.7031\n",
      "Epoch 173/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91562.9844 - mse: 91562.9844 - val_loss: 104760.4688 - val_mse: 104760.4688\n",
      "Epoch 174/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91793.8359 - mse: 91793.8359 - val_loss: 105792.1797 - val_mse: 105792.1797\n",
      "Epoch 175/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 92368.2578 - mse: 92368.2578 - val_loss: 105496.5781 - val_mse: 105496.5781\n",
      "Epoch 176/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91493.7031 - mse: 91493.7031 - val_loss: 104493.7734 - val_mse: 104493.7734\n",
      "Epoch 177/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91198.2812 - mse: 91198.2812 - val_loss: 107893.3047 - val_mse: 107893.3047\n",
      "Epoch 178/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 90245.6641 - mse: 90245.6641 - val_loss: 115947.4609 - val_mse: 115947.4609\n",
      "Epoch 179/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 90526.3594 - mse: 90526.3594 - val_loss: 109964.8828 - val_mse: 109964.8828\n",
      "Epoch 180/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91072.2266 - mse: 91072.2266 - val_loss: 109362.7031 - val_mse: 109362.7031\n",
      "Epoch 181/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91152.8438 - mse: 91152.8438 - val_loss: 105589.6875 - val_mse: 105589.6875\n",
      "Epoch 182/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91339.3438 - mse: 91339.3438 - val_loss: 106794.9609 - val_mse: 106794.9609\n",
      "Epoch 183/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91596.1328 - mse: 91596.1328 - val_loss: 103593.8984 - val_mse: 103593.8984\n",
      "Epoch 184/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 91177.1016 - mse: 91177.1016 - val_loss: 103855.7109 - val_mse: 103855.7109\n",
      "Epoch 185/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 90498.8359 - mse: 90498.8359 - val_loss: 105399.5234 - val_mse: 105399.5234\n",
      "Epoch 186/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89912.5703 - mse: 89912.5703 - val_loss: 104889.3125 - val_mse: 104889.3125\n",
      "Epoch 187/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 90583.6328 - mse: 90583.6328 - val_loss: 102809.5781 - val_mse: 102809.5781\n",
      "Epoch 188/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 90627.7656 - mse: 90627.7656 - val_loss: 106424.6172 - val_mse: 106424.6172\n",
      "Epoch 189/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89866.3984 - mse: 89866.3984 - val_loss: 102911.5391 - val_mse: 102911.5391\n",
      "Epoch 190/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 90291.0469 - mse: 90291.0469 - val_loss: 104989.6797 - val_mse: 104989.6797\n",
      "Epoch 191/500\n",
      "484/484 [==============================] - 1s 3ms/step - loss: 89781.0547 - mse: 89781.0547 - val_loss: 102431.0312 - val_mse: 102431.0312\n",
      "Epoch 192/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89959.5391 - mse: 89959.5391 - val_loss: 103912.0703 - val_mse: 103912.0703\n",
      "Epoch 193/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89780.5938 - mse: 89780.5938 - val_loss: 100947.5625 - val_mse: 100947.5625\n",
      "Epoch 194/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89925.4688 - mse: 89925.4688 - val_loss: 105598.5703 - val_mse: 105598.5703\n",
      "Epoch 195/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89234.6094 - mse: 89234.6094 - val_loss: 104239.8359 - val_mse: 104239.8359\n",
      "Epoch 196/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89866.0547 - mse: 89866.0547 - val_loss: 104633.7031 - val_mse: 104633.7031\n",
      "Epoch 197/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89369.7109 - mse: 89369.7109 - val_loss: 106717.9297 - val_mse: 106717.9297\n",
      "Epoch 198/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89666.8359 - mse: 89666.8359 - val_loss: 104736.8203 - val_mse: 104736.8203\n",
      "Epoch 199/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89456.2188 - mse: 89456.2188 - val_loss: 105791.2422 - val_mse: 105791.2422\n",
      "Epoch 200/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89577.0703 - mse: 89577.0703 - val_loss: 104371.2812 - val_mse: 104371.2812\n",
      "Epoch 201/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89280.9766 - mse: 89280.9766 - val_loss: 107612.2344 - val_mse: 107612.2344\n",
      "Epoch 202/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89503.2891 - mse: 89503.2891 - val_loss: 102866.0312 - val_mse: 102866.0312\n",
      "Epoch 203/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88944.7422 - mse: 88944.7422 - val_loss: 114986.7266 - val_mse: 114986.7266\n",
      "Epoch 204/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88675.0078 - mse: 88675.0078 - val_loss: 104390.6328 - val_mse: 104390.6328\n",
      "Epoch 205/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88074.0156 - mse: 88074.0156 - val_loss: 102618.8750 - val_mse: 102618.8750\n",
      "Epoch 206/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89101.9844 - mse: 89101.9844 - val_loss: 101690.4531 - val_mse: 101690.4531\n",
      "Epoch 207/500\n",
      "484/484 [==============================] - 1s 3ms/step - loss: 88436.0156 - mse: 88436.0156 - val_loss: 105480.6641 - val_mse: 105480.6641\n",
      "Epoch 208/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88134.1484 - mse: 88134.1484 - val_loss: 104363.5938 - val_mse: 104363.5938\n",
      "Epoch 209/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88640.7266 - mse: 88640.7266 - val_loss: 108289.7266 - val_mse: 108289.7266\n",
      "Epoch 210/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88826.7656 - mse: 88826.7656 - val_loss: 102202.6719 - val_mse: 102202.6719\n",
      "Epoch 211/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88739.0000 - mse: 88739.0000 - val_loss: 102188.4375 - val_mse: 102188.4375\n",
      "Epoch 212/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88000.5078 - mse: 88000.5078 - val_loss: 107533.2969 - val_mse: 107533.2969\n",
      "Epoch 213/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 89198.7734 - mse: 89198.7734 - val_loss: 101826.8828 - val_mse: 101826.8828\n",
      "Epoch 214/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87999.9375 - mse: 87999.9375 - val_loss: 104797.1172 - val_mse: 104797.1172\n",
      "Epoch 215/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88523.2188 - mse: 88523.2188 - val_loss: 108427.5625 - val_mse: 108427.5625\n",
      "Epoch 216/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88147.5391 - mse: 88147.5391 - val_loss: 101998.2578 - val_mse: 101998.2578\n",
      "Epoch 217/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88114.2031 - mse: 88114.2031 - val_loss: 112960.6484 - val_mse: 112960.6484\n",
      "Epoch 218/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88845.8359 - mse: 88845.8359 - val_loss: 101357.6797 - val_mse: 101357.6797\n",
      "Epoch 219/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88069.4844 - mse: 88069.4844 - val_loss: 103567.5469 - val_mse: 103567.5469\n",
      "Epoch 220/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87093.1172 - mse: 87093.1172 - val_loss: 105413.2812 - val_mse: 105413.2812\n",
      "Epoch 221/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88150.9141 - mse: 88150.9141 - val_loss: 99655.6328 - val_mse: 99655.6328\n",
      "Epoch 222/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88330.1797 - mse: 88330.1797 - val_loss: 101963.5078 - val_mse: 101963.5078\n",
      "Epoch 223/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88252.9531 - mse: 88252.9531 - val_loss: 101946.2500 - val_mse: 101946.2500\n",
      "Epoch 224/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88077.3125 - mse: 88077.3125 - val_loss: 101757.6953 - val_mse: 101757.6953\n",
      "Epoch 225/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87581.5312 - mse: 87581.5312 - val_loss: 106521.1797 - val_mse: 106521.1797\n",
      "Epoch 226/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88024.8672 - mse: 88024.8672 - val_loss: 104885.0156 - val_mse: 104885.0156\n",
      "Epoch 227/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88457.9766 - mse: 88457.9766 - val_loss: 101198.9062 - val_mse: 101198.9062\n",
      "Epoch 228/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87711.4375 - mse: 87711.4375 - val_loss: 110679.6484 - val_mse: 110679.6484\n",
      "Epoch 229/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 88211.6172 - mse: 88211.6172 - val_loss: 101352.2344 - val_mse: 101352.2344\n",
      "Epoch 230/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87262.2969 - mse: 87262.2969 - val_loss: 100669.1875 - val_mse: 100669.1875\n",
      "Epoch 231/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86883.2344 - mse: 86883.2344 - val_loss: 99761.0234 - val_mse: 99761.0234\n",
      "Epoch 232/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87050.2578 - mse: 87050.2578 - val_loss: 109679.5859 - val_mse: 109679.5859\n",
      "Epoch 233/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86705.0391 - mse: 86705.0391 - val_loss: 102234.7188 - val_mse: 102234.7188\n",
      "Epoch 234/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87330.1875 - mse: 87330.1875 - val_loss: 101609.2969 - val_mse: 101609.2969\n",
      "Epoch 235/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86981.5156 - mse: 86981.5156 - val_loss: 103952.3672 - val_mse: 103952.3672\n",
      "Epoch 236/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87128.7266 - mse: 87128.7266 - val_loss: 108523.9609 - val_mse: 108523.9609\n",
      "Epoch 237/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87630.2578 - mse: 87630.2578 - val_loss: 107800.8281 - val_mse: 107800.8281\n",
      "Epoch 238/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86995.5000 - mse: 86995.5000 - val_loss: 102925.6250 - val_mse: 102925.6250\n",
      "Epoch 239/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87953.3828 - mse: 87953.3828 - val_loss: 100536.8516 - val_mse: 100536.8516\n",
      "Epoch 240/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86489.0000 - mse: 86489.0000 - val_loss: 101574.5938 - val_mse: 101574.5938\n",
      "Epoch 241/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87606.6250 - mse: 87606.6250 - val_loss: 101303.5938 - val_mse: 101303.5938\n",
      "Epoch 242/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86734.8203 - mse: 86734.8203 - val_loss: 104253.0391 - val_mse: 104253.0391\n",
      "Epoch 243/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86650.9375 - mse: 86650.9375 - val_loss: 98538.2109 - val_mse: 98538.2109\n",
      "Epoch 244/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87356.0391 - mse: 87356.0391 - val_loss: 100706.7344 - val_mse: 100706.7344\n",
      "Epoch 245/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86919.5547 - mse: 86919.5547 - val_loss: 102219.1953 - val_mse: 102219.1953\n",
      "Epoch 246/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87272.5312 - mse: 87272.5312 - val_loss: 101560.4375 - val_mse: 101560.4375\n",
      "Epoch 247/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86879.0078 - mse: 86879.0078 - val_loss: 98321.4297 - val_mse: 98321.4297\n",
      "Epoch 248/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86571.1797 - mse: 86571.1797 - val_loss: 102736.3047 - val_mse: 102736.3047\n",
      "Epoch 249/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86168.4375 - mse: 86168.4375 - val_loss: 105791.1875 - val_mse: 105791.1875\n",
      "Epoch 250/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86698.0469 - mse: 86698.0469 - val_loss: 101572.3516 - val_mse: 101572.3516\n",
      "Epoch 251/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87051.6641 - mse: 87051.6641 - val_loss: 101975.0391 - val_mse: 101975.0391\n",
      "Epoch 252/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86906.5078 - mse: 86906.5078 - val_loss: 102690.4609 - val_mse: 102690.4609\n",
      "Epoch 253/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86399.4375 - mse: 86399.4375 - val_loss: 102973.5625 - val_mse: 102973.5625\n",
      "Epoch 254/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86658.7422 - mse: 86658.7422 - val_loss: 101547.9375 - val_mse: 101547.9375\n",
      "Epoch 255/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85756.6016 - mse: 85756.6016 - val_loss: 100159.5234 - val_mse: 100159.5234\n",
      "Epoch 256/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86048.3906 - mse: 86048.3906 - val_loss: 112419.2266 - val_mse: 112419.2266\n",
      "Epoch 257/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86566.4609 - mse: 86566.4609 - val_loss: 103556.9375 - val_mse: 103556.9375\n",
      "Epoch 258/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 87760.9844 - mse: 87760.9844 - val_loss: 102616.6797 - val_mse: 102616.6797\n",
      "Epoch 259/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86640.5078 - mse: 86640.5078 - val_loss: 101427.6797 - val_mse: 101427.6797\n",
      "Epoch 260/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86933.3516 - mse: 86933.3516 - val_loss: 102536.4297 - val_mse: 102536.4297\n",
      "Epoch 261/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86503.1641 - mse: 86503.1641 - val_loss: 103944.1562 - val_mse: 103944.1562\n",
      "Epoch 262/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86038.6172 - mse: 86038.6172 - val_loss: 102225.8047 - val_mse: 102225.8047\n",
      "Epoch 263/500\n",
      "484/484 [==============================] - 1s 3ms/step - loss: 86179.0234 - mse: 86179.0234 - val_loss: 99991.2578 - val_mse: 99991.2578\n",
      "Epoch 264/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86506.0625 - mse: 86506.0625 - val_loss: 103827.9375 - val_mse: 103827.9375\n",
      "Epoch 265/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86182.1875 - mse: 86182.1875 - val_loss: 101352.8984 - val_mse: 101352.8984\n",
      "Epoch 266/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86062.3672 - mse: 86062.3672 - val_loss: 103661.7578 - val_mse: 103661.7578\n",
      "Epoch 267/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85578.9609 - mse: 85578.9609 - val_loss: 110874.8828 - val_mse: 110874.8828\n",
      "Epoch 268/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86241.9219 - mse: 86241.9219 - val_loss: 102972.7031 - val_mse: 102972.7031\n",
      "Epoch 269/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86016.0625 - mse: 86016.0625 - val_loss: 100675.8047 - val_mse: 100675.8047\n",
      "Epoch 270/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85914.7578 - mse: 85914.7578 - val_loss: 100192.4844 - val_mse: 100192.4844\n",
      "Epoch 271/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85190.0547 - mse: 85190.0547 - val_loss: 98649.6484 - val_mse: 98649.6484\n",
      "Epoch 272/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86269.5781 - mse: 86269.5781 - val_loss: 99085.2656 - val_mse: 99085.2656\n",
      "Epoch 273/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86042.5547 - mse: 86042.5547 - val_loss: 106596.8516 - val_mse: 106596.8516\n",
      "Epoch 274/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86838.8906 - mse: 86838.8906 - val_loss: 97379.1641 - val_mse: 97379.1641\n",
      "Epoch 275/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86192.4453 - mse: 86192.4453 - val_loss: 99609.9297 - val_mse: 99609.9297\n",
      "Epoch 276/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85864.0234 - mse: 85864.0234 - val_loss: 107014.2578 - val_mse: 107014.2578\n",
      "Epoch 277/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86400.3984 - mse: 86400.3984 - val_loss: 103993.2031 - val_mse: 103993.2031\n",
      "Epoch 278/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85933.0078 - mse: 85933.0078 - val_loss: 105630.6094 - val_mse: 105630.6094\n",
      "Epoch 279/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86570.7109 - mse: 86570.7109 - val_loss: 100567.7578 - val_mse: 100567.7578\n",
      "Epoch 280/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84908.0078 - mse: 84908.0078 - val_loss: 104279.7578 - val_mse: 104279.7578\n",
      "Epoch 281/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85582.2812 - mse: 85582.2812 - val_loss: 109131.9219 - val_mse: 109131.9219\n",
      "Epoch 282/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85630.2344 - mse: 85630.2344 - val_loss: 98511.8125 - val_mse: 98511.8125\n",
      "Epoch 283/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86678.1719 - mse: 86678.1719 - val_loss: 101969.0078 - val_mse: 101969.0078\n",
      "Epoch 284/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86385.4219 - mse: 86385.4219 - val_loss: 98680.9219 - val_mse: 98680.9219\n",
      "Epoch 285/500\n",
      "484/484 [==============================] - 1s 3ms/step - loss: 85520.6094 - mse: 85520.6094 - val_loss: 103509.2578 - val_mse: 103509.2578\n",
      "Epoch 286/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86757.6719 - mse: 86757.6719 - val_loss: 100774.9062 - val_mse: 100774.9062\n",
      "Epoch 287/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85309.7188 - mse: 85309.7188 - val_loss: 100780.3203 - val_mse: 100780.3203\n",
      "Epoch 288/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85013.2812 - mse: 85013.2812 - val_loss: 101529.1094 - val_mse: 101529.1094\n",
      "Epoch 289/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85655.3047 - mse: 85655.3047 - val_loss: 102299.5938 - val_mse: 102299.5938\n",
      "Epoch 290/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85638.3438 - mse: 85638.3438 - val_loss: 99572.0078 - val_mse: 99572.0078\n",
      "Epoch 291/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85716.2656 - mse: 85716.2656 - val_loss: 99787.0000 - val_mse: 99787.0000\n",
      "Epoch 292/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85946.5703 - mse: 85946.5703 - val_loss: 101648.4766 - val_mse: 101648.4766\n",
      "Epoch 293/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85548.9609 - mse: 85548.9609 - val_loss: 107475.6328 - val_mse: 107475.6328\n",
      "Epoch 294/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85917.5000 - mse: 85917.5000 - val_loss: 106011.6328 - val_mse: 106011.6328\n",
      "Epoch 295/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 86048.4297 - mse: 86048.4297 - val_loss: 101597.2969 - val_mse: 101597.2969\n",
      "Epoch 296/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84782.1797 - mse: 84782.1797 - val_loss: 98271.1797 - val_mse: 98271.1797\n",
      "Epoch 297/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85712.9219 - mse: 85712.9219 - val_loss: 105800.5703 - val_mse: 105800.5703\n",
      "Epoch 298/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85390.8828 - mse: 85390.8828 - val_loss: 102186.1719 - val_mse: 102186.1719\n",
      "Epoch 299/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85603.1172 - mse: 85603.1172 - val_loss: 102208.8594 - val_mse: 102208.8594\n",
      "Epoch 300/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85071.7422 - mse: 85071.7422 - val_loss: 104587.6094 - val_mse: 104587.6094\n",
      "Epoch 301/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85697.3438 - mse: 85697.3438 - val_loss: 106954.7188 - val_mse: 106954.7188\n",
      "Epoch 302/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85622.0078 - mse: 85622.0078 - val_loss: 103220.7578 - val_mse: 103220.7578\n",
      "Epoch 303/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84537.7734 - mse: 84537.7734 - val_loss: 102190.7031 - val_mse: 102190.7031\n",
      "Epoch 304/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85078.6406 - mse: 85078.6406 - val_loss: 104863.7578 - val_mse: 104863.7578\n",
      "Epoch 305/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85141.8594 - mse: 85141.8594 - val_loss: 100208.5312 - val_mse: 100208.5312\n",
      "Epoch 306/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84792.6172 - mse: 84792.6172 - val_loss: 97607.1641 - val_mse: 97607.1641\n",
      "Epoch 307/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84989.4922 - mse: 84989.4922 - val_loss: 99360.8750 - val_mse: 99360.8750\n",
      "Epoch 308/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85183.8906 - mse: 85183.8906 - val_loss: 100004.9766 - val_mse: 100004.9766\n",
      "Epoch 309/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85306.2266 - mse: 85306.2266 - val_loss: 98295.7578 - val_mse: 98295.7578\n",
      "Epoch 310/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84873.1250 - mse: 84873.1250 - val_loss: 103615.2734 - val_mse: 103615.2734\n",
      "Epoch 311/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85171.6641 - mse: 85171.6641 - val_loss: 102606.8516 - val_mse: 102606.8516\n",
      "Epoch 312/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85511.7344 - mse: 85511.7344 - val_loss: 99646.9688 - val_mse: 99646.9688\n",
      "Epoch 313/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85344.8203 - mse: 85344.8203 - val_loss: 96652.2578 - val_mse: 96652.2578\n",
      "Epoch 314/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84316.6484 - mse: 84316.6484 - val_loss: 111543.8828 - val_mse: 111543.8828\n",
      "Epoch 315/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85298.7266 - mse: 85298.7266 - val_loss: 100950.1406 - val_mse: 100950.1406\n",
      "Epoch 316/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85210.3281 - mse: 85210.3281 - val_loss: 110433.3906 - val_mse: 110433.3906\n",
      "Epoch 317/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85232.7969 - mse: 85232.7969 - val_loss: 99914.1094 - val_mse: 99914.1094\n",
      "Epoch 318/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85339.7500 - mse: 85339.7500 - val_loss: 109647.4844 - val_mse: 109647.4844\n",
      "Epoch 319/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84837.8203 - mse: 84837.8203 - val_loss: 105996.2656 - val_mse: 105996.2656\n",
      "Epoch 320/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85237.0469 - mse: 85237.0469 - val_loss: 99145.8516 - val_mse: 99145.8516\n",
      "Epoch 321/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85022.9375 - mse: 85022.9375 - val_loss: 98959.2891 - val_mse: 98959.2891\n",
      "Epoch 322/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84037.1094 - mse: 84037.1094 - val_loss: 110350.9453 - val_mse: 110350.9453\n",
      "Epoch 323/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85426.0000 - mse: 85426.0000 - val_loss: 101612.2344 - val_mse: 101612.2344\n",
      "Epoch 324/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85223.8125 - mse: 85223.8125 - val_loss: 102915.5781 - val_mse: 102915.5781\n",
      "Epoch 325/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84672.0312 - mse: 84672.0312 - val_loss: 105577.4609 - val_mse: 105577.4609\n",
      "Epoch 326/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85451.5078 - mse: 85451.5078 - val_loss: 98862.6328 - val_mse: 98862.6328\n",
      "Epoch 327/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84823.4609 - mse: 84823.4609 - val_loss: 98655.0156 - val_mse: 98655.0156\n",
      "Epoch 328/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84552.2578 - mse: 84552.2578 - val_loss: 99197.9844 - val_mse: 99197.9844\n",
      "Epoch 329/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84034.8672 - mse: 84034.8672 - val_loss: 106812.6562 - val_mse: 106812.6562\n",
      "Epoch 330/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84971.3906 - mse: 84971.3906 - val_loss: 102580.0703 - val_mse: 102580.0703\n",
      "Epoch 331/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84850.6094 - mse: 84850.6094 - val_loss: 100545.5078 - val_mse: 100545.5078\n",
      "Epoch 332/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84837.2109 - mse: 84837.2109 - val_loss: 100684.8047 - val_mse: 100684.8047\n",
      "Epoch 333/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84732.0703 - mse: 84732.0703 - val_loss: 98628.9844 - val_mse: 98628.9844\n",
      "Epoch 334/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84044.7266 - mse: 84044.7266 - val_loss: 99270.7344 - val_mse: 99270.7344\n",
      "Epoch 335/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84771.9297 - mse: 84771.9297 - val_loss: 107070.6016 - val_mse: 107070.6016\n",
      "Epoch 336/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83786.5078 - mse: 83786.5078 - val_loss: 99164.0078 - val_mse: 99164.0078\n",
      "Epoch 337/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84313.2344 - mse: 84313.2344 - val_loss: 98589.9062 - val_mse: 98589.9062\n",
      "Epoch 338/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85424.8203 - mse: 85424.8203 - val_loss: 107666.7656 - val_mse: 107666.7656\n",
      "Epoch 339/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85235.4531 - mse: 85235.4531 - val_loss: 97364.9141 - val_mse: 97364.9141\n",
      "Epoch 340/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84102.3125 - mse: 84102.3125 - val_loss: 109333.8906 - val_mse: 109333.8906\n",
      "Epoch 341/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85047.1953 - mse: 85047.1953 - val_loss: 109247.6953 - val_mse: 109247.6953\n",
      "Epoch 342/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84617.6328 - mse: 84617.6328 - val_loss: 99920.8750 - val_mse: 99920.8750\n",
      "Epoch 343/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84755.0000 - mse: 84755.0000 - val_loss: 103545.7500 - val_mse: 103545.7500\n",
      "Epoch 344/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83748.6484 - mse: 83748.6484 - val_loss: 102953.5469 - val_mse: 102953.5469\n",
      "Epoch 345/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84794.2188 - mse: 84794.2188 - val_loss: 100791.3125 - val_mse: 100791.3125\n",
      "Epoch 346/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84784.5625 - mse: 84784.5625 - val_loss: 99569.0469 - val_mse: 99569.0469\n",
      "Epoch 347/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84666.0938 - mse: 84666.0938 - val_loss: 99842.0156 - val_mse: 99842.0156\n",
      "Epoch 348/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 85018.9531 - mse: 85018.9531 - val_loss: 105705.7344 - val_mse: 105705.7344\n",
      "Epoch 349/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84569.6562 - mse: 84569.6562 - val_loss: 99529.7422 - val_mse: 99529.7422\n",
      "Epoch 350/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84679.3516 - mse: 84679.3516 - val_loss: 104467.1484 - val_mse: 104467.1484\n",
      "Epoch 351/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84912.5547 - mse: 84912.5547 - val_loss: 100063.6953 - val_mse: 100063.6953\n",
      "Epoch 352/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84990.9219 - mse: 84990.9219 - val_loss: 101130.2266 - val_mse: 101130.2266\n",
      "Epoch 353/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84289.1875 - mse: 84289.1875 - val_loss: 100680.1484 - val_mse: 100680.1484\n",
      "Epoch 354/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84267.8438 - mse: 84267.8438 - val_loss: 99906.4531 - val_mse: 99906.4531\n",
      "Epoch 355/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84729.6484 - mse: 84729.6484 - val_loss: 97705.9062 - val_mse: 97705.9062\n",
      "Epoch 356/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84213.6250 - mse: 84213.6250 - val_loss: 98780.1484 - val_mse: 98780.1484\n",
      "Epoch 357/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84497.0469 - mse: 84497.0469 - val_loss: 98743.6641 - val_mse: 98743.6641\n",
      "Epoch 358/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83849.4297 - mse: 83849.4297 - val_loss: 97914.1172 - val_mse: 97914.1172\n",
      "Epoch 359/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84236.5391 - mse: 84236.5391 - val_loss: 103586.3906 - val_mse: 103586.3906\n",
      "Epoch 360/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84034.8047 - mse: 84034.8047 - val_loss: 100830.6484 - val_mse: 100830.6484\n",
      "Epoch 361/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83914.2500 - mse: 83914.2500 - val_loss: 99881.8594 - val_mse: 99881.8594\n",
      "Epoch 362/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84296.1094 - mse: 84296.1094 - val_loss: 102285.2266 - val_mse: 102285.2266\n",
      "Epoch 363/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84741.7500 - mse: 84741.7500 - val_loss: 99433.8984 - val_mse: 99433.8984\n",
      "Epoch 364/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84846.5625 - mse: 84846.5625 - val_loss: 101077.9844 - val_mse: 101077.9844\n",
      "Epoch 365/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83672.7656 - mse: 83672.7656 - val_loss: 100393.9844 - val_mse: 100393.9844\n",
      "Epoch 366/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84185.7031 - mse: 84185.7031 - val_loss: 100176.0703 - val_mse: 100176.0703\n",
      "Epoch 367/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84203.6719 - mse: 84203.6719 - val_loss: 106856.9844 - val_mse: 106856.9844\n",
      "Epoch 368/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83951.5781 - mse: 83951.5781 - val_loss: 97664.5938 - val_mse: 97664.5938\n",
      "Epoch 369/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83665.6719 - mse: 83665.6719 - val_loss: 101541.1406 - val_mse: 101541.1406\n",
      "Epoch 370/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83886.3359 - mse: 83886.3359 - val_loss: 107836.7969 - val_mse: 107836.7969\n",
      "Epoch 371/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84481.4531 - mse: 84481.4531 - val_loss: 100054.7422 - val_mse: 100054.7422\n",
      "Epoch 372/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83857.1562 - mse: 83857.1562 - val_loss: 97349.0469 - val_mse: 97349.0469\n",
      "Epoch 373/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83545.5703 - mse: 83545.5703 - val_loss: 101740.9453 - val_mse: 101740.9453\n",
      "Epoch 374/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84905.6562 - mse: 84905.6562 - val_loss: 102676.0625 - val_mse: 102676.0625\n",
      "Epoch 375/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83929.7500 - mse: 83929.7500 - val_loss: 102033.4219 - val_mse: 102033.4219\n",
      "Epoch 376/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84485.6484 - mse: 84485.6484 - val_loss: 99083.0938 - val_mse: 99083.0938\n",
      "Epoch 377/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83489.3047 - mse: 83489.3047 - val_loss: 100384.2734 - val_mse: 100384.2734\n",
      "Epoch 378/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84270.2656 - mse: 84270.2656 - val_loss: 100759.5703 - val_mse: 100759.5703\n",
      "Epoch 379/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83951.8281 - mse: 83951.8281 - val_loss: 99729.0547 - val_mse: 99729.0547\n",
      "Epoch 380/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84105.1562 - mse: 84105.1562 - val_loss: 100599.9844 - val_mse: 100599.9844\n",
      "Epoch 381/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84074.9766 - mse: 84074.9766 - val_loss: 107370.9453 - val_mse: 107370.9453\n",
      "Epoch 382/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84061.3906 - mse: 84061.3906 - val_loss: 102172.1016 - val_mse: 102172.1016\n",
      "Epoch 383/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83942.7422 - mse: 83942.7422 - val_loss: 98698.8594 - val_mse: 98698.8594\n",
      "Epoch 384/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83845.7891 - mse: 83845.7891 - val_loss: 96892.2969 - val_mse: 96892.2969\n",
      "Epoch 385/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84453.8984 - mse: 84453.8984 - val_loss: 99896.2891 - val_mse: 99896.2891\n",
      "Epoch 386/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84287.6875 - mse: 84287.6875 - val_loss: 98936.2188 - val_mse: 98936.2188\n",
      "Epoch 387/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84567.4219 - mse: 84567.4219 - val_loss: 104328.1172 - val_mse: 104328.1172\n",
      "Epoch 388/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84263.2578 - mse: 84263.2578 - val_loss: 96557.7188 - val_mse: 96557.7188\n",
      "Epoch 389/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83187.4688 - mse: 83187.4688 - val_loss: 104045.3750 - val_mse: 104045.3750\n",
      "Epoch 390/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83724.8516 - mse: 83724.8516 - val_loss: 97231.6328 - val_mse: 97231.6328\n",
      "Epoch 391/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84031.1719 - mse: 84031.1719 - val_loss: 97489.6797 - val_mse: 97489.6797\n",
      "Epoch 392/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83814.5000 - mse: 83814.5000 - val_loss: 98614.2656 - val_mse: 98614.2656\n",
      "Epoch 393/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84019.7266 - mse: 84019.7266 - val_loss: 102251.1406 - val_mse: 102251.1406\n",
      "Epoch 394/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84035.0547 - mse: 84035.0547 - val_loss: 99464.7891 - val_mse: 99464.7891\n",
      "Epoch 395/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83782.4062 - mse: 83782.4062 - val_loss: 99942.7031 - val_mse: 99942.7031\n",
      "Epoch 396/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83560.8438 - mse: 83560.8438 - val_loss: 101572.8359 - val_mse: 101572.8359\n",
      "Epoch 397/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83322.6016 - mse: 83322.6016 - val_loss: 99933.2109 - val_mse: 99933.2109\n",
      "Epoch 398/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84015.8672 - mse: 84015.8672 - val_loss: 98737.6016 - val_mse: 98737.6016\n",
      "Epoch 399/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84884.2656 - mse: 84884.2656 - val_loss: 98166.9922 - val_mse: 98166.9922\n",
      "Epoch 400/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83815.3438 - mse: 83815.3438 - val_loss: 98620.0234 - val_mse: 98620.0234\n",
      "Epoch 401/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83834.4141 - mse: 83834.4141 - val_loss: 96031.6250 - val_mse: 96031.6250\n",
      "Epoch 402/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83097.4688 - mse: 83097.4688 - val_loss: 98190.3516 - val_mse: 98190.3516\n",
      "Epoch 403/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83891.9766 - mse: 83891.9766 - val_loss: 97143.3984 - val_mse: 97143.3984\n",
      "Epoch 404/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83359.1953 - mse: 83359.1953 - val_loss: 98965.6641 - val_mse: 98965.6641\n",
      "Epoch 405/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83460.9453 - mse: 83460.9453 - val_loss: 98212.3125 - val_mse: 98212.3125\n",
      "Epoch 406/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83780.3516 - mse: 83780.3516 - val_loss: 100691.0156 - val_mse: 100691.0156\n",
      "Epoch 407/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84053.4922 - mse: 84053.4922 - val_loss: 102816.8359 - val_mse: 102816.8359\n",
      "Epoch 408/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83584.8984 - mse: 83584.8984 - val_loss: 108639.4922 - val_mse: 108639.4922\n",
      "Epoch 409/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83429.1562 - mse: 83429.1562 - val_loss: 97441.1406 - val_mse: 97441.1406\n",
      "Epoch 410/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83592.0000 - mse: 83592.0000 - val_loss: 99226.2344 - val_mse: 99226.2344\n",
      "Epoch 411/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84490.6016 - mse: 84490.6016 - val_loss: 99739.6094 - val_mse: 99739.6094\n",
      "Epoch 412/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84462.6328 - mse: 84462.6328 - val_loss: 108039.7266 - val_mse: 108039.7266\n",
      "Epoch 413/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83263.6953 - mse: 83263.6953 - val_loss: 103003.7578 - val_mse: 103003.7578\n",
      "Epoch 414/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83656.8125 - mse: 83656.8125 - val_loss: 96891.8047 - val_mse: 96891.8047\n",
      "Epoch 415/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84003.0078 - mse: 84003.0078 - val_loss: 97031.8047 - val_mse: 97031.8047\n",
      "Epoch 416/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83230.3984 - mse: 83230.3984 - val_loss: 104758.1875 - val_mse: 104758.1875\n",
      "Epoch 417/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83539.2500 - mse: 83539.2500 - val_loss: 102818.9688 - val_mse: 102818.9688\n",
      "Epoch 418/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83646.0547 - mse: 83646.0547 - val_loss: 110518.8594 - val_mse: 110518.8594\n",
      "Epoch 419/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84843.4844 - mse: 84843.4844 - val_loss: 103683.8203 - val_mse: 103683.8203\n",
      "Epoch 420/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82795.4062 - mse: 82795.4062 - val_loss: 102855.4844 - val_mse: 102855.4844\n",
      "Epoch 421/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84082.0156 - mse: 84082.0156 - val_loss: 99696.6797 - val_mse: 99696.6797\n",
      "Epoch 422/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83191.7578 - mse: 83191.7578 - val_loss: 99075.8438 - val_mse: 99075.8438\n",
      "Epoch 423/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83431.2578 - mse: 83431.2578 - val_loss: 102143.0000 - val_mse: 102143.0000\n",
      "Epoch 424/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84215.8594 - mse: 84215.8594 - val_loss: 101327.2891 - val_mse: 101327.2891\n",
      "Epoch 425/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84199.7812 - mse: 84199.7812 - val_loss: 101290.5859 - val_mse: 101290.5859\n",
      "Epoch 426/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83184.3359 - mse: 83184.3359 - val_loss: 99983.6797 - val_mse: 99983.6797\n",
      "Epoch 427/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84104.7188 - mse: 84104.7188 - val_loss: 105010.7109 - val_mse: 105010.7109\n",
      "Epoch 428/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83657.4219 - mse: 83657.4219 - val_loss: 102393.3125 - val_mse: 102393.3125\n",
      "Epoch 429/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83265.8828 - mse: 83265.8828 - val_loss: 102617.8203 - val_mse: 102617.8203\n",
      "Epoch 430/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83334.7188 - mse: 83334.7188 - val_loss: 98867.5312 - val_mse: 98867.5312\n",
      "Epoch 431/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83053.7422 - mse: 83053.7422 - val_loss: 99585.6328 - val_mse: 99585.6328\n",
      "Epoch 432/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83484.6719 - mse: 83484.6719 - val_loss: 99499.5547 - val_mse: 99499.5547\n",
      "Epoch 433/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83540.0078 - mse: 83540.0078 - val_loss: 97262.5703 - val_mse: 97262.5703\n",
      "Epoch 434/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83223.1953 - mse: 83223.1953 - val_loss: 97287.4922 - val_mse: 97287.4922\n",
      "Epoch 435/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82930.1797 - mse: 82930.1797 - val_loss: 98972.1016 - val_mse: 98972.1016\n",
      "Epoch 436/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84032.2422 - mse: 84032.2422 - val_loss: 100423.8281 - val_mse: 100423.8281\n",
      "Epoch 437/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82715.9922 - mse: 82715.9922 - val_loss: 98012.3125 - val_mse: 98012.3125\n",
      "Epoch 438/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84300.5391 - mse: 84300.5391 - val_loss: 95994.4688 - val_mse: 95994.4688\n",
      "Epoch 439/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83165.7188 - mse: 83165.7188 - val_loss: 96433.6797 - val_mse: 96433.6797\n",
      "Epoch 440/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83296.9062 - mse: 83296.9062 - val_loss: 100752.5078 - val_mse: 100752.5078\n",
      "Epoch 441/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82383.8281 - mse: 82383.8281 - val_loss: 101207.2578 - val_mse: 101207.2578\n",
      "Epoch 442/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83412.7969 - mse: 83412.7969 - val_loss: 98570.3359 - val_mse: 98570.3359\n",
      "Epoch 443/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83541.4766 - mse: 83541.4766 - val_loss: 102438.4609 - val_mse: 102438.4609\n",
      "Epoch 444/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83674.4766 - mse: 83674.4766 - val_loss: 96895.8125 - val_mse: 96895.8125\n",
      "Epoch 445/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82525.4531 - mse: 82525.4531 - val_loss: 97780.2500 - val_mse: 97780.2500\n",
      "Epoch 446/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83199.2500 - mse: 83199.2500 - val_loss: 98864.3984 - val_mse: 98864.3984\n",
      "Epoch 447/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83083.9688 - mse: 83083.9688 - val_loss: 103396.6562 - val_mse: 103396.6562\n",
      "Epoch 448/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82733.2969 - mse: 82733.2969 - val_loss: 97123.0000 - val_mse: 97123.0000\n",
      "Epoch 449/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82821.0156 - mse: 82821.0156 - val_loss: 98154.9453 - val_mse: 98154.9453\n",
      "Epoch 450/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83401.5781 - mse: 83401.5781 - val_loss: 97080.2734 - val_mse: 97080.2734\n",
      "Epoch 451/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82957.8594 - mse: 82957.8594 - val_loss: 97667.9922 - val_mse: 97667.9922\n",
      "Epoch 452/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83379.7812 - mse: 83379.7812 - val_loss: 99279.7031 - val_mse: 99279.7031\n",
      "Epoch 453/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83023.8047 - mse: 83023.8047 - val_loss: 97654.5391 - val_mse: 97654.5391\n",
      "Epoch 454/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83097.3828 - mse: 83097.3828 - val_loss: 100078.3516 - val_mse: 100078.3516\n",
      "Epoch 455/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83128.5312 - mse: 83128.5312 - val_loss: 97432.1328 - val_mse: 97432.1328\n",
      "Epoch 456/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82870.6562 - mse: 82870.6562 - val_loss: 111895.8203 - val_mse: 111895.8203\n",
      "Epoch 457/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83177.7344 - mse: 83177.7344 - val_loss: 102008.5312 - val_mse: 102008.5312\n",
      "Epoch 458/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83320.4609 - mse: 83320.4609 - val_loss: 99189.4766 - val_mse: 99189.4766\n",
      "Epoch 459/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83269.7656 - mse: 83269.7656 - val_loss: 95436.6641 - val_mse: 95436.6641\n",
      "Epoch 460/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83747.8828 - mse: 83747.8828 - val_loss: 95810.8750 - val_mse: 95810.8750\n",
      "Epoch 461/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82531.8047 - mse: 82531.8047 - val_loss: 100850.7188 - val_mse: 100850.7188\n",
      "Epoch 462/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82287.3047 - mse: 82287.3047 - val_loss: 96332.1172 - val_mse: 96332.1172\n",
      "Epoch 463/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83302.1406 - mse: 83302.1406 - val_loss: 97709.3125 - val_mse: 97709.3125\n",
      "Epoch 464/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83015.9609 - mse: 83015.9609 - val_loss: 96841.4766 - val_mse: 96841.4766\n",
      "Epoch 465/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82415.4219 - mse: 82415.4219 - val_loss: 104527.6875 - val_mse: 104527.6875\n",
      "Epoch 466/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82041.5391 - mse: 82041.5391 - val_loss: 96618.4297 - val_mse: 96618.4297\n",
      "Epoch 467/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 81744.5703 - mse: 81744.5703 - val_loss: 111976.4766 - val_mse: 111976.4766\n",
      "Epoch 468/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82763.4922 - mse: 82763.4922 - val_loss: 103453.1641 - val_mse: 103453.1641\n",
      "Epoch 469/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83057.2812 - mse: 83057.2812 - val_loss: 97379.9922 - val_mse: 97379.9922\n",
      "Epoch 470/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 84055.4219 - mse: 84055.4219 - val_loss: 97559.9219 - val_mse: 97559.9219\n",
      "Epoch 471/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82707.5469 - mse: 82707.5469 - val_loss: 96624.5469 - val_mse: 96624.5469\n",
      "Epoch 472/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82460.7500 - mse: 82460.7500 - val_loss: 99091.0703 - val_mse: 99091.0703\n",
      "Epoch 473/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82491.6328 - mse: 82491.6328 - val_loss: 99426.6953 - val_mse: 99426.6953\n",
      "Epoch 474/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82665.5859 - mse: 82665.5859 - val_loss: 98485.4062 - val_mse: 98485.4062\n",
      "Epoch 475/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82939.8281 - mse: 82939.8281 - val_loss: 98393.5547 - val_mse: 98393.5547\n",
      "Epoch 476/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82778.6719 - mse: 82778.6719 - val_loss: 94665.0078 - val_mse: 94665.0078\n",
      "Epoch 477/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82467.8828 - mse: 82467.8828 - val_loss: 99513.7734 - val_mse: 99513.7734\n",
      "Epoch 478/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82445.3281 - mse: 82445.3281 - val_loss: 101634.3750 - val_mse: 101634.3750\n",
      "Epoch 479/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82590.7734 - mse: 82590.7734 - val_loss: 96544.6875 - val_mse: 96544.6875\n",
      "Epoch 480/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82717.3047 - mse: 82717.3047 - val_loss: 104356.2656 - val_mse: 104356.2656\n",
      "Epoch 481/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82016.6875 - mse: 82016.6875 - val_loss: 99756.2969 - val_mse: 99756.2969\n",
      "Epoch 482/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83187.3438 - mse: 83187.3438 - val_loss: 97417.6328 - val_mse: 97417.6328\n",
      "Epoch 483/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83518.1094 - mse: 83518.1094 - val_loss: 97527.3438 - val_mse: 97527.3438\n",
      "Epoch 484/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82965.1641 - mse: 82965.1641 - val_loss: 99978.2422 - val_mse: 99978.2422\n",
      "Epoch 485/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 81559.0781 - mse: 81559.0781 - val_loss: 96505.8281 - val_mse: 96505.8281\n",
      "Epoch 486/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82766.6875 - mse: 82766.6875 - val_loss: 104152.5938 - val_mse: 104152.5938\n",
      "Epoch 487/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82049.1094 - mse: 82049.1094 - val_loss: 99077.5547 - val_mse: 99077.5547\n",
      "Epoch 488/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82887.4844 - mse: 82887.4844 - val_loss: 97791.5938 - val_mse: 97791.5938\n",
      "Epoch 489/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 81904.3359 - mse: 81904.3359 - val_loss: 109537.2656 - val_mse: 109537.2656\n",
      "Epoch 490/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82380.4531 - mse: 82380.4531 - val_loss: 97016.1328 - val_mse: 97016.1328\n",
      "Epoch 491/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82269.9453 - mse: 82269.9453 - val_loss: 97964.5391 - val_mse: 97964.5391\n",
      "Epoch 492/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82388.8359 - mse: 82388.8359 - val_loss: 101968.2188 - val_mse: 101968.2188\n",
      "Epoch 493/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 81898.9219 - mse: 81898.9219 - val_loss: 102725.4766 - val_mse: 102725.4766\n",
      "Epoch 494/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82894.7422 - mse: 82894.7422 - val_loss: 98009.8750 - val_mse: 98009.8750\n",
      "Epoch 495/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82420.3516 - mse: 82420.3516 - val_loss: 101841.7344 - val_mse: 101841.7344\n",
      "Epoch 496/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 83057.1094 - mse: 83057.1094 - val_loss: 100214.2734 - val_mse: 100214.2734\n",
      "Epoch 497/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 81760.4609 - mse: 81760.4609 - val_loss: 99639.3750 - val_mse: 99639.3750\n",
      "Epoch 498/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82891.7891 - mse: 82891.7891 - val_loss: 97709.0156 - val_mse: 97709.0156\n",
      "Epoch 499/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82168.3984 - mse: 82168.3984 - val_loss: 98420.9297 - val_mse: 98420.9297\n",
      "Epoch 500/500\n",
      "484/484 [==============================] - 1s 2ms/step - loss: 82252.8047 - mse: 82252.8047 - val_loss: 99830.3359 - val_mse: 99830.3359\n",
      "121/121 [==============================] - 0s 905us/step\n",
      "Mean Absolute Error: 168.45097717458685\n",
      "Mean Squared Error: 315.9594088685122\n",
      "R^2 Score: 0.9880311997671355\n"
     ]
    }
   ],
   "source": [
    "# 98 % accuracy for high epoch\n",
    "\n",
    "X = df_1.drop(['sm', 'ttime'], axis=1)\n",
    "y = df_1['sm']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "outputs = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32)\n",
    "\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions,squared=False))\n",
    "print(\"R^2 Score:\", r2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
